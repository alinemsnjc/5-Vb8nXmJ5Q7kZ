{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alinemsnjc/5-Vb8nXmJ5Q7kZ/blob/main/C%C3%B3pia_de_Imers%C3%A3o_Agentes_de_IA_Alura_%2B_Google_Gemini_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aula 01"
      ],
      "metadata": {
        "id": "7SlD7rndaQVD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHslr4rgVWVL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8808b6a3-67ca-4199-8181-0466c80a120f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q --upgrade langchain langchain-google-genai google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importação da API Key"
      ],
      "metadata": {
        "id": "KS8dxquvaWMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GEMINI_API_KEY')"
      ],
      "metadata": {
        "id": "2ODE-m2nSIMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conexão com o Gemini"
      ],
      "metadata": {
        "id": "pZrwAO37aYga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    temperature=0.0,\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "NNm7FXjXYR-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resp_test = llm.invoke(\"Quem é você? Seja criativo.\")\n",
        "print(resp_test.content)"
      ],
      "metadata": {
        "id": "0G7bN2ynabhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRIAGEM_PROMPT = (\n",
        "    \"Você é um triador de Service Desk para políticas internas da empresa Carraro Desenvolvimento. \"\n",
        "    \"Dada a mensagem do usuário, retorne SOMENTE um JSON com:\\n\"\n",
        "    \"{\\n\"\n",
        "    '  \"decisao\": \"AUTO_RESOLVER\" | \"PEDIR_INFO\" | \"ABRIR_CHAMADO\",\\n'\n",
        "    '  \"urgencia\": \"BAIXA\" | \"MEDIA\" | \"ALTA\",\\n'\n",
        "    '  \"campos_faltantes\": [\"...\"]\\n'\n",
        "    \"}\\n\"\n",
        "    \"Regras:\\n\"\n",
        "    '- **AUTO_RESOLVER**: Perguntas claras sobre regras ou procedimentos descritos nas políticas (Ex: \"Posso reembolsar a internet do meu home office?\", \"Como funciona a política de alimentação em viagens?\").\\n'\n",
        "    '- **PEDIR_INFO**: Mensagens vagas ou que faltam informações para identificar o tema ou contexto (Ex: \"Preciso de ajuda com uma política\", \"Tenho uma dúvida geral\").\\n'\n",
        "    '- **ABRIR_CHAMADO**: Pedidos de exceção, liberação, aprovação ou acesso especial, ou quando o usuário explicitamente pede para abrir um chamado (Ex: \"Quero exceção para trabalhar 5 dias remoto.\", \"Solicito liberação para anexos externos.\", \"Por favor, abra um chamado para o RH.\").'\n",
        "    \"Analise a mensagem e decida a ação mais apropriada.\"\n",
        ")"
      ],
      "metadata": {
        "id": "jDfTiUi0bSv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import Literal, List, Dict\n",
        "\n",
        "class TriagemOut(BaseModel):\n",
        "    decisao: Literal[\"AUTO_RESOLVER\", \"PEDIR_INFO\", \"ABRIR_CHAMADO\"]\n",
        "    urgencia: Literal[\"BAIXA\", \"MEDIA\", \"ALTA\"]\n",
        "    campos_faltantes: List[str] = Field(default_factory=list)"
      ],
      "metadata": {
        "id": "OzUoKnnxe3Tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_triagem = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    temperature=0.0,\n",
        "    api_key=GOOGLE_API_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "DB0qKzkjgUJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "triagem_chain = llm_triagem.with_structured_output(TriagemOut)\n",
        "\n",
        "def triagem(mensagem: str) -> Dict:\n",
        "    saida: TriagemOut = triagem_chain.invoke([\n",
        "        SystemMessage(content=TRIAGEM_PROMPT),\n",
        "        HumanMessage(content=mensagem)\n",
        "    ])\n",
        "\n",
        "    return saida.model_dump()"
      ],
      "metadata": {
        "id": "ljP2Kf6xggJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testes = [\"Posso reembolsar a internet?\",\n",
        "          \"Quero mais 5 dias de trabalho remoto. Como faço?\",\n",
        "          \"Posso reembolsar cursos ou treinamentos da Alura?\",\n",
        "          \"Quantas capivaras tem no Rio Pinheiros?\"]"
      ],
      "metadata": {
        "id": "-OR0GRuyrO9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for msg_teste in testes:\n",
        "    print(f\"Pergunta: {msg_teste}\\n -> Resposta: {triagem(msg_teste)}\\n\")"
      ],
      "metadata": {
        "id": "MK4KoXYkr0hX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aula 02"
      ],
      "metadata": {
        "id": "IrzMICHUyowD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade langchain_community faiss-cpu langchain-text-splitters pymupdf"
      ],
      "metadata": {
        "id": "23tyECmJyqhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "docs = []\n",
        "\n",
        "for n in Path(\"/content/\").glob(\"*.pdf\"):\n",
        "    try:\n",
        "        loader = PyMuPDFLoader(str(n))\n",
        "        docs.extend(loader.load())\n",
        "        print(f\"Carregado com sucesso arquivo {n.name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao carregar arquivo {n.name}: {e}\")\n",
        "\n",
        "print(f\"Total de documentos carregados: {len(docs)}\")"
      ],
      "metadata": {
        "id": "UlgCNmm2zeY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)\n",
        "\n",
        "chunks = splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "smC_v8Dp2oCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este código utiliza o RecursiveCharacterTextSplitter da biblioteca langchain_text_splitters para dividir os documentos carregados em pedaços menores (chunks).\n",
        "\n",
        "Aqui está o que cada parte faz:\n",
        "\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter: Importa a classe RecursiveCharacterTextSplitter.\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30): Cria uma instância do splitter.\n",
        "chunk_size=300: Define o tamanho máximo de cada pedaço em 300 caracteres.\n",
        "chunk_overlap=30: Define uma sobreposição de 30 caracteres entre os pedaços. Isso ajuda a manter o contexto entre eles.\n",
        "chunks = splitter.split_documents(docs): Divide a lista de documentos (docs) em pedaços menores (chunks) usando o splitter configurado.\n",
        "Essa divisão é útil para processar documentos longos, pois permite que você trabalhe com partes menores do texto, o que é importante para muitas tarefas de processamento de linguagem natural, como a criação de embeddings para busca de similaridade.\n"
      ],
      "metadata": {
        "id": "-NdffUhGWpnN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ikn3dRp0WY_W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este trecho de código carrega documentos PDF do diretório /content/ usando o PyMuPDFLoader da biblioteca langchain_community.document_loaders.\n",
        "\n",
        "Aqui está uma explicação detalhada:\n",
        "\n",
        "from pathlib import Path: Importa a classe Path para trabalhar com caminhos de arquivo.\n",
        "from langchain_community.document_loaders import PyMuPDFLoader: Importa o PyMuPDFLoader para carregar documentos PDF.\n",
        "docs = []: Inicializa uma lista vazia para armazenar os documentos carregados.\n",
        "for n in Path(\"/content/\").glob(\"*.pdf\"):: Este loop itera por todos os arquivos que terminam com .pdf no diretório /content/.\n",
        "loader = PyMuPDFLoader(str(n)): Cria uma instância do PyMuPDFLoader para cada arquivo PDF encontrado.\n",
        "docs.extend(loader.load()): Carrega o conteúdo do arquivo PDF usando o loader e o adiciona à lista docs.\n",
        "print(f\"Carregado com sucesso arquivo {n.name}\"): Imprime uma mensagem de sucesso para cada arquivo carregado.\n",
        "except Exception as e:: Este bloco lida com quaisquer erros que possam ocorrer durante o processo de carregamento.\n",
        "print(f\"Erro ao carregar arquivo {n.name}: {e}\"): Imprime uma mensagem de erro se um arquivo não for carregado.\n",
        "print(f\"Total de documentos carregados: {len(docs)}\"): Finalmente, imprime o número total de documentos carregados."
      ],
      "metadata": {
        "id": "DXJNavKSVYtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in chunks:\n",
        "    print(chunk)\n",
        "    print(\"------------------------------------\")"
      ],
      "metadata": {
        "id": "C3ng3KaX330w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este código simplesmente itera sobre a lista de \"chunks\" (os pedaços de texto gerados pelo RecursiveCharacterTextSplitter) e imprime o conteúdo de cada um, seguido por uma linha de separação (------------------------------------).\n",
        "\n",
        "É útil para visualizar como os documentos foram divididos em pedaços menores e verificar se a divisão ocorreu conforme o esperado."
      ],
      "metadata": {
        "id": "_6VjiCaycAlE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hPlmYq0tbTBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/gemini-embedding-001\",\n",
        "    google_api_key=GOOGLE_API_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "LkxrYFKG4dqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este código inicializa o modelo de embeddings do Google Gemini para criar representações vetoriais dos seus dados.\n",
        "\n",
        "Aqui está o que cada parte faz:\n",
        "\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings: Importa a classe GoogleGenerativeAIEmbeddings.\n",
        "embeddings = GoogleGenerativeAIEmbeddings(...): Cria uma instância da classe GoogleGenerativeAIEmbeddings.\n",
        "model=\"models/gemini-embedding-001\": Especifica o modelo de embedding a ser utilizado, neste caso, o \"gemini-embedding-001\".\n",
        "google_api_key=GOOGLE_API_KEY: Passa a sua chave de API do Google para autenticação. É importante que a variável GOOGLE_API_KEY contenha a sua chave de API carregada de forma segura (como feito anteriormente no notebook usando userdata.get).\n",
        "O objeto embeddings criado pode ser usado para converter texto em vetores numéricos, que são essenciais para tarefas como busca de similaridade em bancos de dados vetoriais.\n",
        "\n"
      ],
      "metadata": {
        "id": "IaL-hYDpeaAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_type=\"similarity_score_threshold\",\n",
        "                                     search_kwargs={\"score_threshold\":0.3, \"k\": 4})"
      ],
      "metadata": {
        "id": "FtEw2T-47zhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Este código demonstra como criar um armazenamento vetorial (vectorstore) usando FAISS e configurar um retriever para buscar documentos relevantes com base em embeddings.\n",
        "\n",
        "Aqui está o que cada parte faz:\n",
        "\n",
        "from langchain_community.vectorstores import FAISS: Importa a classe FAISS da biblioteca langchain_community.vectorstores. FAISS é uma biblioteca para busca de similaridade eficiente e agrupamento de vetores densos.\n",
        "vectorstore = FAISS.from_documents(chunks, embeddings): Cria uma instância do FAISS vectorstore a partir dos chunks (os pedaços de texto processados) e dos embeddings (as representações vetoriais desses chunks). Isso constrói o índice de busca no FAISS.\n",
        "retriever = vectorstore.as_retriever(...): Converte o vectorstore FAISS em um objeto retriever. Um retriever é responsável por buscar documentos relevantes dado uma consulta.\n",
        "search_type=\"similarity_score_threshold\": Define o tipo de busca como busca por similaridade com um limite de pontuação. Isso significa que apenas documentos com uma pontuação de similaridade acima de um certo limite serão retornados.\n",
        "search_kwargs={\"score_threshold\":0.3, \"k\": 4}: Define argumentos adicionais para a busca.\n",
        "\"score_threshold\":0.3: Define o limite mínimo de pontuação de similaridade para que um documento seja considerado relevante. Documentos com pontuação abaixo de 0.3 serão ignorados.\n",
        "\"k\": 4: Define o número máximo de documentos mais relevantes a serem retornados após a aplicação do limite de pontuação.\n",
        "Em resumo, este código configura um sistema onde você pode dar uma consulta (pergunta), o retriever a converterá em um embedding, buscará no índice FAISS por chunks de documentos com embeddings semelhantes acima de um certo nível de similaridade, e retornará os chunks mais relevantes."
      ],
      "metadata": {
        "id": "bc99WPzzgDmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "prompt_rag = ChatPromptTemplate.from_messages([\n",
        "    (\"system\",\n",
        "     \"Você é um Assistente de Políticas Internas (RH/IT) da empresa Carraro Desenvolvimento. \"\n",
        "     \"Responda SOMENTE com base no contexto fornecido. \"\n",
        "     \"Se não houver base suficiente, responda apenas 'Não sei'.\"),\n",
        "\n",
        "    (\"human\", \"Pergunta: {input}\\n\\nContexto:\\n{context}\")\n",
        "])\n",
        "\n",
        "document_chain = create_stuff_documents_chain(llm_triagem, prompt_rag)"
      ],
      "metadata": {
        "id": "Pq5f11bq99Zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este código define um template de prompt para um modelo de linguagem e cria uma chain (cadeia) para combinar documentos com esse prompt.\n",
        "\n",
        "Aqui está o que cada parte faz:\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate: Importa a classe ChatPromptTemplate para criar prompts formatados para modelos de chat.\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain: Importa a função create_stuff_documents_chain para criar uma chain que combina documentos com um prompt.\n",
        "prompt_rag = ChatPromptTemplate.from_messages([...]): Cria uma instância de ChatPromptTemplate.\n",
        "(\"system\", ...): Define uma mensagem do sistema que instrui o modelo sobre seu papel e comportamento (ser um Assistente de Políticas Internas e responder apenas com base no contexto fornecido).\n",
        "(\"human\", \"Pergunta: {input}\\n\\nContexto:\\n{context}\"): Define o formato da mensagem do usuário, incluindo um placeholder {input} para a pergunta do usuário e um placeholder {context} para o contexto relevante extraído dos documentos.\n",
        "document_chain = create_stuff_documents_chain(llm_triagem, prompt_rag): Cria a chain que combina os documentos. Ela recebe o modelo de linguagem (llm_triagem) e o template de prompt (prompt_rag). Quando esta chain é invocada, ela insere o contexto dos documentos relevantes no prompt e envia para o modelo de linguagem para gerar a resposta.\n",
        "Em resumo, este código prepara o sistema para usar o contexto de documentos recuperados (pelo retriever) para responder a perguntas do usuário de forma informada."
      ],
      "metadata": {
        "id": "KeS0QzDWh2bw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Formatadores\n",
        "import re, pathlib\n",
        "\n",
        "def _clean_text(s: str) -> str:\n",
        "    return re.sub(r\"\\s+\", \" \", s or \"\").strip()\n",
        "\n",
        "def extrair_trecho(texto: str, query: str, janela: int = 240) -> str:\n",
        "    txt = _clean_text(texto)\n",
        "    termos = [t.lower() for t in re.findall(r\"\\w+\", query or \"\") if len(t) >= 4]\n",
        "    pos = -1\n",
        "    for t in termos:\n",
        "        pos = txt.lower().find(t)\n",
        "        if pos != -1: break\n",
        "    if pos == -1: pos = 0\n",
        "    ini, fim = max(0, pos - janela//2), min(len(txt), pos + janela//2)\n",
        "    return txt[ini:fim]\n",
        "\n",
        "def formatar_citacoes(docs_rel: List, query: str) -> List[Dict]:\n",
        "    cites, seen = [], set()\n",
        "    for d in docs_rel:\n",
        "        src = pathlib.Path(d.metadata.get(\"source\",\"\")).name\n",
        "        page = int(d.metadata.get(\"page\", 0)) + 1\n",
        "        key = (src, page)\n",
        "        if key in seen:\n",
        "            continue\n",
        "        seen.add(key)\n",
        "        cites.append({\"documento\": src, \"pagina\": page, \"trecho\": extrair_trecho(d.page_content, query)})\n",
        "    return cites[:3]"
      ],
      "metadata": {
        "id": "TFXl4XW5GiXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perguntar_politica_RAG(pergunta: str) -> Dict:\n",
        "    docs_relacionados = retriever.invoke(pergunta)\n",
        "\n",
        "    if not docs_relacionados:\n",
        "        return {\"answer\": \"Não sei.\",\n",
        "                \"citacoes\": [],\n",
        "                \"contexto_encontrado\": False}\n",
        "\n",
        "    answer = document_chain.invoke({\"input\": pergunta,\n",
        "                                    \"context\": docs_relacionados})\n",
        "\n",
        "    txt = (answer or \"\").strip()\n",
        "\n",
        "    if txt.rstrip(\".!?\") == \"Não sei\":\n",
        "        return {\"answer\": \"Não sei.\",\n",
        "                \"citacoes\": [],\n",
        "                \"contexto_encontrado\": False}\n",
        "\n",
        "    return {\"answer\": txt,\n",
        "            \"citacoes\": formatar_citacoes(docs_relacionados, pergunta),\n",
        "            \"contexto_encontrado\": True}"
      ],
      "metadata": {
        "id": "LIEL-CI4CgTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testes = [\"Posso reembolsar a internet?\",\n",
        "          \"Quero mais 5 dias de trabalho remoto. Como faço?\",\n",
        "          \"Posso reembolsar cursos ou treinamentos da Alura?\",\n",
        "          \"Quantas capivaras tem no Rio Pinheiros?\"]"
      ],
      "metadata": {
        "id": "Tj5lYEgxFREl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for msg_teste in testes:\n",
        "    resposta = perguntar_politica_RAG(msg_teste)\n",
        "    print(f\"PERGUNTA: {msg_teste}\")\n",
        "    print(f\"RESPOSTA: {resposta['answer']}\")\n",
        "    if resposta['contexto_encontrado']:\n",
        "        print(\"CITAÇÕES:\")\n",
        "        for c in resposta['citacoes']:\n",
        "            print(f\" - Documento: {c['documento']}, Página: {c['pagina']}\")\n",
        "            print(f\"   Trecho: {c['trecho']}\")\n",
        "        print(\"------------------------------------\")"
      ],
      "metadata": {
        "id": "qXgjzhqnFWOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TvYHxWbWGj8M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aula 03"
      ],
      "metadata": {
        "id": "__9gaaNXcTwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade langgraph"
      ],
      "metadata": {
        "id": "JQFXYkI-GoHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalando o Langgraph"
      ],
      "metadata": {
        "id": "rE5bLj1TG97v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, Optional\n",
        "\n",
        "class AgentState(TypedDict, total = False):\n",
        "    pergunta: str\n",
        "    triagem: dict\n",
        "    resposta: Optional[str]\n",
        "    citacoes: List[dict]\n",
        "    rag_sucesso: bool\n",
        "    acao_final: str"
      ],
      "metadata": {
        "id": "oBbGV7-xHJhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este código define uma classe Python AgentState que é usada para representar o estado de um agente dentro de um fluxo de trabalho. Ele utiliza TypedDict do módulo typing para fornecer dicas de tipo, o que ajuda a entender a estrutura esperada do estado.\n",
        "\n",
        "Aqui está um detalhe de cada parte:\n",
        "\n",
        "from typing import TypedDict, Optional:\n",
        "Esta linha importa TypedDict e Optional do módulo typing. TypedDict é usado para criar dicionários com chaves e tipos de valor específicos, e Optional indica que um valor pode ser de um tipo especificado ou None.\n",
        "class AgentState(TypedDict, total=False)::\n",
        "Isso define a classe AgentState, herdando de TypedDict. O argumento total=False significa que nem todas as chaves no dicionário são obrigatórias.\n",
        "pergunta: str: Isso indica que o estado pode ter uma chave chamada pergunta que é esperada ser uma string.\n",
        "triagem: dict: Isso indica uma chave chamada triagem que é esperada ser um dicionário.\n",
        "resposta: Optional[str]: Isso indica uma chave chamada resposta que pode ser uma string ou None.\n",
        "citacoes: List[dict]: Isso indica uma chave chamada citacoes que é esperada ser uma lista de dicionários.\n",
        "rag_sucesso: bool:\n",
        " Isso indica uma chave chamada rag_sucesso que é esperada ser um valor booleano.\n",
        "acao_final: str: Isso indica uma chave chamada acao_final que é esperada ser uma string.\n",
        "Em resumo, a classe AgentState funciona como um modelo para um dicionário que armazena várias informações relacionadas ao processo do agente, como a pergunta do usuário (pergunta), o resultado de uma etapa de triagem (triagem), a resposta gerada (resposta), citações relevantes (citacoes), se uma etapa RAG (Retrieval Augmented Generation) foi bem-sucedida (rag_sucesso) e a ação final tomada (acao_final)."
      ],
      "metadata": {
        "id": "GEV9fx87H3GD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def node_triagem(state: AgentState) -> AgentState:\n",
        "    print(\"Executando nó de triagem...\")\n",
        "    return {\"triagem\": triagem(state[\"pergunta\"])}"
      ],
      "metadata": {
        "id": "RfK3M6WWJYZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta função parece ser um nó em um fluxo de trabalho ou grafo de estado, como o que você pode construir com a biblioteca langgraph que foi mencionada anteriormente.\n",
        "\n",
        "Aqui está o que a função faz:\n",
        "\n",
        "def node_triagem(state: AgentState) -> AgentState:: Define a função chamada node_triagem que aceita um argumento state do tipo AgentState (a classe que você definou anteriormente para representar o estado do agente) e retorna um valor do mesmo tipo AgentState.\n",
        "print(\"Executando nó de triagem...\"): Imprime uma mensagem no console indicando que este nó está sendo executado. Isso é útil para depuração e para acompanhar o fluxo do programa.\n",
        "return {\"triagem\": triagem(state[\"pergunta\"])}: Esta é a parte principal da função.\n",
        "Ele chama a função triagem (que você definiu em uma célula anterior para realizar a triagem da pergunta do usuário).\n",
        "Passa a state[\"pergunta\"] como argumento para a função triagem. Isso significa que a função de triagem usará a pergunta armazenada no estado atual do agente.\n",
        "Retorna um dicionário contendo a chave \"triagem\" com o resultado da execução da função triagem. Este dicionário será usado para atualizar o estado do agente no fluxo de trabalho.\n",
        "Em resumo, esta função node_triagem pega o estado atual do agente, extrai a pergunta, passa essa pergunta para a função de triagem, e retorna o resultado da triagem para ser adicionado ao estado do agente. É um passo fundamental na sua pipeline para determinar a natureza da solicitação do usuário.\n",
        "\n"
      ],
      "metadata": {
        "id": "UOikU4zkKMkb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como mencionei anteriormente, os próximos passos no seu notebook envolveriam:\n",
        "\n",
        "Definir as funções restantes para os nós do seu grafo Langgraph:\n",
        " Você já tem a função node_triagem. Provavelmente precisará de funções para lidar com o RAG (Retrieval Augmented Generation), talvez uma para lidar com casos que precisam de mais informações (PEDIR_INFO), e outra para formatar a resposta final.\n",
        "Construir e compilar o grafo Langgraph:\n",
        " Conectar as funções definidas em um fluxo lógico usando a estrutura do langgraph. Isso define como o estado do agente muda entre os diferentes nós com base nas decisões tomadas (por exemplo, o resultado da triagem).\n",
        "Executar e testar o grafo completo: Testar o fluxo de ponta a ponta com diferentes tipos de perguntas para garantir que o agente está se comportando conforme o esperado e tomando as ações corretas em cada cenário."
      ],
      "metadata": {
        "id": "9AEHaZQRKY2p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def node_auto_resolver(state: AgentState) -> AgentState:\n",
        "    print(\"Executando nó de auto_resolver...\")\n",
        "    resposta_rag = perguntar_politica_RAG(state[\"pergunta\"])\n",
        "\n",
        "    update: AgentState = {\n",
        "        \"resposta\": resposta_rag[\"answer\"],\n",
        "        \"citacoes\": resposta_rag.get(\"citacoes\", []),\n",
        "        \"rag_sucesso\": resposta_rag[\"contexto_encontrado\"],\n",
        "    }\n",
        "\n",
        "    if resposta_rag[\"contexto_encontrado\"]:\n",
        "        update[\"acao_final\"] = \"AUTO_RESOLVER\"\n",
        "\n",
        "    return update"
      ],
      "metadata": {
        "id": "Dqo19EMxKiZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def node_pedir_info(state: AgentState) -> AgentState:\n",
        "    print(\"Executando nó de pedir_info...\")\n",
        "    faltantes = state[\"triagem\"].get(\"campos_faltantes\", [])\n",
        "    if faltantes:\n",
        "        detalhe = \",\".join(faltantes)\n",
        "    else:\n",
        "        detalhe = \"Tema e contexto específico\"\n",
        "\n",
        "    return {\n",
        "        \"resposta\": f\"Para avançar, preciso que detalhe: {detalhe}\",\n",
        "        \"citacoes\": [],\n",
        "        \"acao_final\": \"PEDIR_INFO\"\n",
        "    }"
      ],
      "metadata": {
        "id": "I1UusT4xK0iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def node_abrir_chamado(state: AgentState) -> AgentState:\n",
        "    print(\"Executando nó de abrir_chamado...\")\n",
        "    triagem = state[\"triagem\"]\n",
        "\n",
        "    return {\n",
        "        \"resposta\": f\"Abrindo chamado com urgência {triagem['urgencia']}. Descrição: {state['pergunta'][:140]}\",\n",
        "        \"citacoes\": [],\n",
        "        \"acao_final\": \"ABRIR_CHAMADO\"\n",
        "    }"
      ],
      "metadata": {
        "id": "DBorO2jULEgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KEYWORDS_ABRIR_TICKET = [\"aprovação\", \"exceção\", \"liberação\", \"abrir ticket\", \"abrir chamado\", \"acesso especial\"]\n",
        "\n",
        "def decidir_pos_triagem(state: AgentState) -> str:\n",
        "    print(\"Decidindo após a triagem...\")\n",
        "    decisao = state[\"triagem\"][\"decisao\"]\n",
        "\n",
        "    if decisao == \"AUTO_RESOLVER\": return \"auto\"\n",
        "    if decisao == \"PEDIR_INFO\": return \"info\"\n",
        "    if decisao == \"ABRIR_CHAMADO\": return \"chamado\""
      ],
      "metadata": {
        "id": "IQhcSv2ZLYju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decidir_pos_auto_resolver(state: AgentState) -> str:\n",
        "    print(\"Decidindo após o auto_resolver...\")\n",
        "\n",
        "    if state.get(\"rag_sucesso\"):\n",
        "        print(\"Rag com sucesso, finalizando o fluxo.\")\n",
        "        return \"ok\"\n",
        "\n",
        "    state_da_pergunta = (state[\"pergunta\"] or \"\").lower()\n",
        "\n",
        "    if any(k in state_da_pergunta for k in KEYWORDS_ABRIR_TICKET):\n",
        "        print(\"Rag falhou, mas foram encontradas keywords de abertura de ticket. Abrindo...\")\n",
        "        return \"chamado\"\n",
        "\n",
        "    print(\"Rag falhou, sem keywords, vou pedir mais informações...\")\n",
        "    return \"info\""
      ],
      "metadata": {
        "id": "4wTo-IfDLp7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "workflow.add_node(\"triagem\", node_triagem)\n",
        "workflow.add_node(\"auto_resolver\", node_auto_resolver)\n",
        "workflow.add_node(\"pedir_info\", node_pedir_info)\n",
        "workflow.add_node(\"abrir_chamado\", node_abrir_chamado)\n",
        "\n",
        "workflow.add_edge(START, \"triagem\")\n",
        "workflow.add_conditional_edges(\"triagem\", decidir_pos_triagem, {\n",
        "    \"auto\": \"auto_resolver\",\n",
        "    \"info\": \"pedir_info\",\n",
        "    \"chamado\": \"abrir_chamado\"\n",
        "})\n",
        "\n",
        "workflow.add_conditional_edges(\"auto_resolver\", decidir_pos_auto_resolver, {\n",
        "    \"info\": \"pedir_info\",\n",
        "    \"chamado\": \"abrir_chamado\",\n",
        "    \"ok\": END\n",
        "})\n",
        "\n",
        "workflow.add_edge(\"pedir_info\", END)\n",
        "workflow.add_edge(\"abrir_chamado\", END)\n",
        "\n",
        "grafo = workflow.compile()"
      ],
      "metadata": {
        "id": "21rEkphqL_cF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, este código constrói e compila o grafo do Langgraph que define o fluxo do seu agente.\n",
        "\n",
        "Vamos detalhar cada parte:\n",
        "\n",
        "from langgraph.graph import StateGraph, START, END: Importa as classes e constantes necessárias da biblioteca langgraph.graph.\n",
        "StateGraph: A classe principal para definir um grafo de estado.\n",
        "START: Uma constante que representa o nó inicial do grafo.\n",
        "END: Uma constante que representa o nó final do grafo.\n",
        "workflow = StateGraph(AgentState): Cria uma instância de StateGraph. Ele recebe AgentState como argumento, indicando que este grafo operará sobre um estado que segue a estrutura definida pela classe AgentState que você criou anteriormente.\n",
        "workflow.add_node(\"triagem\", node_triagem): Adiciona um nó ao grafo.\n",
        "O primeiro argumento \"triagem\" é o nome único deste nó dentro do grafo.\n",
        "O segundo argumento node_triagem é a função Python que será executada quando o fluxo chegar a este nó. Esta é a função que você definiu anteriormente para realizar a triagem.\n",
        "workflow.add_node(\"auto_resolver\", node_auto_resolver): Adiciona o nó chamado \"auto_resolver\" associado à função node_auto_resolver.\n",
        "workflow.add_node(\"pedir_info\", node_pedir_info): Adiciona o nó chamado \"pedir_info\" associado à função node_pedir_info.\n",
        "workflow.add_node(\"abrir_chamado\", node_abrir_chamado): Adiciona o nó chamado \"abrir_chamado\" associado à função node_abrir_chamado.\n",
        "workflow.add_edge(START, \"triagem\"): Define uma aresta (transição) no grafo.\n",
        "Isso especifica que o fluxo começa no ponto START e transita diretamente para o nó \"triagem\".\n",
        "workflow.add_conditional_edges(\"triagem\", decidir_pos_triagem, { ... }): Adiciona arestas condicionais a partir do nó \"triagem\".\n",
        "O primeiro argumento \"triagem\" é o nó de onde as transições partem.\n",
        "O segundo argumento decidir_pos_triagem é a função \"decisora\". Esta função (que você definiu anteriormente) examinará o estado atual e retornará uma string que determina para qual nó o fluxo deve ir em seguida.\n",
        "O terceiro argumento é um dicionário que mapeia os possíveis retornos da função decidir_pos_triagem para os nomes dos nós de destino.\n",
        "Se decidir_pos_triagem retornar \"auto\", o fluxo vai para \"auto_resolver\".\n",
        "Se retornar \"info\", vai para \"pedir_info\".\n",
        "Se retornar \"chamado\", vai para \"abrir_chamado\".\n",
        "workflow.add_conditional_edges(\"auto_resolver\", decidir_pos_auto_resolver, { ... }): Adiciona arestas condicionais a partir do nó \"auto_resolver\".\n",
        "Usa a função decisora decidir_pos_auto_resolver para determinar o próximo nó com base no resultado do processo de auto-resolução (RAG).\n",
        "As transições possíveis são para \"pedir_info\", \"abrir_chamado\", ou END (fim do fluxo), dependendo do retorno da função decisora.\n",
        "workflow.add_edge(\"pedir_info\", END): Adiciona uma aresta que leva do nó \"pedir_info\" diretamente para o fim do fluxo (END).\n",
        "workflow.add_edge(\"abrir_chamado\", END): Adiciona uma aresta que leva do nó \"abrir_chamado\" diretamente para o fim do fluxo (END).\n",
        "grafo = workflow.compile(): Compila o grafo que foi definido. Este passo finaliza a estrutura do grafo e o prepara para ser executado. O objeto grafo resultante é o que você usará para invocar o fluxo do agente com uma pergunta inicial.\n",
        "Em resumo, este código monta a arquitetura do seu agente, definindo os passos (nós) e as regras para mover entre eles (arestas e arestas condicionais), criando um fluxo lógico para processar as perguntas do usuário."
      ],
      "metadata": {
        "id": "eqRyrYt9Mppf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Image\n",
        "\n",
        "graph_bytes = grafo.get_graph().draw_mermaid_png()\n",
        "display(Image(graph_bytes))"
      ],
      "metadata": {
        "id": "Q9kdtmnGM1D7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Este código utiliza a biblioteca langgraph para gerar uma representação visual do grafo que você acabou de construir e a exibe no notebook.\n",
        "\n",
        "Aqui está o que cada parte faz:\n",
        "\n",
        "from IPython.display import display, Image: Importa as funções display e Image do módulo IPython.display. Estas funções são usadas no ambiente Colab (e Jupyter) para exibir objetos de forma rica, como imagens.\n",
        "graph_bytes = grafo.get_graph().draw_mermaid_png(): Esta é a parte principal que gera a imagem do grafo.\n",
        "grafo.get_graph(): A partir do objeto grafo compilado, este método obtém a representação interna do grafo.\n",
        ".draw_mermaid_png(): Este método, disponível na representação do grafo, gera uma imagem do grafo no formato PNG, utilizando a sintaxe do Mermaid (uma linguagem de diagrama e visualização). A imagem é retornada como bytes.\n",
        "display(Image(graph_bytes)): Exibe a imagem gerada no notebook.\n",
        "Image(graph_bytes): Cria um objeto Image a partir dos bytes da imagem PNG gerada.\n",
        "display(...): A função display renderiza este objeto Image no output da célula.\n",
        "Em resumo, este código é uma ferramenta visual útil para verificar a estrutura do grafo que você definiu, garantindo que as arestas e nós estejam conectados da forma esperada antes de executá-lo. A visualização em formato Mermaid PNG torna o fluxo do agente mais fácil de entender e depurar."
      ],
      "metadata": {
        "id": "G4BjnBVzNabP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testes = [\"Posso reembolsar a internet?\",\n",
        "          \"Quero mais 5 dias de trabalho remoto. Como faço?\",\n",
        "          \"Posso reembolsar cursos ou treinamentos da Alura?\",\n",
        "          \"É possível reembolsar certificações do Google Cloud?\",\n",
        "          \"Posso obter o Google Gemini de graça?\",\n",
        "          \"Qual é a palavra-chave da aula de hoje?\",\n",
        "          \"Quantas capivaras tem no Rio Pinheiros?\"]"
      ],
      "metadata": {
        "id": "vG116OJOO4OQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for msg_test in testes:\n",
        "    resposta_final = grafo.invoke({\"pergunta\": msg_test})\n",
        "\n",
        "    triag = resposta_final.get(\"triagem\", {})\n",
        "    print(f\"PERGUNTA: {msg_test}\")\n",
        "    print(f\"DECISÃO: {triag.get('decisao')} | URGÊNCIA: {triag.get('urgencia')} | AÇÃO FINAL: {resposta_final.get('acao_final')}\")\n",
        "    print(f\"RESPOSTA: {resposta_final.get('resposta')}\")\n",
        "    if resposta_final.get(\"citacoes\"):\n",
        "        print(\"CITAÇÕES:\")\n",
        "        for citacao in resposta_final.get(\"citacoes\"):\n",
        "            print(f\" - Documento: {citacao['documento']}, Página: {citacao['pagina']}\")\n",
        "            print(f\"   Trecho: {citacao['trecho']}\")\n",
        "\n",
        "    print(\"------------------------------------\")"
      ],
      "metadata": {
        "id": "Q4pEX5N9O_7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código selecionado executa o grafo do Langgraph que você construiu com uma lista de perguntas de teste e imprime os resultados.\n",
        "\n",
        "Vamos detalhar o que cada parte faz:\n",
        "\n",
        "for msg_test in testes:: Este loop itera sobre a lista testes que contém várias strings, onde cada string é uma pergunta de teste.\n",
        "resposta_final = grafo.invoke({\"pergunta\": msg_test}): Esta é a linha crucial onde o grafo é executado.\n",
        "grafo.invoke(...): Invoca o grafo compilado.\n",
        "{\"pergunta\": msg_test}: Passa o estado inicial para o grafo. Neste caso, o estado inicial é um dicionário com a chave \"pergunta\" cujo valor é a pergunta de teste atual (msg_test). O grafo começa a processar a partir do ponto START com este estado inicial.\n",
        "resposta_final: Armazena o estado final do grafo após a execução. Este estado final conterá todas as atualizações feitas pelos nós do grafo (como o resultado da triagem, a resposta gerada, etc.).\n",
        "triag = resposta_final.get(\"triagem\", {}): Tenta obter o resultado da triagem do estado final (resposta_final). Se a chave \"triagem\" não existir no estado final (o que não deve acontecer neste grafo, mas é uma boa prática usar .get()), ele retorna um dicionário vazio {} como padrão.\n",
        "print(f\"PERGUNTA: {msg_test}\"): Imprime a pergunta de teste que foi processada.\n",
        "print(f\"DECISÃO: {triag.get('decisao')} | URGÊNCIA: {triag.get('urgencia')} | AÇÃO FINAL: {resposta_final.get('acao_final')}\"): Imprime informações importantes sobre o resultado da execução do grafo para aquela pergunta:\n",
        "triag.get('decisao'): A decisão de triagem (AUTO_RESOLVER, PEDIR_INFO, ABRIR_CHAMADO).\n",
        "triag.get('urgencia'): A urgência determinada pela triagem (BAIXA, MEDIA, ALTA).\n",
        "resposta_final.get('acao_final'): A ação final tomada pelo grafo (AUTO_RESOLVER, PEDIR_INFO, ABRIR_CHAMADO), que é definida pelos nós finais do fluxo.\n",
        "print(f\"RESPOSTA: {resposta_final.get('resposta')}\"): Imprime a resposta gerada pelo grafo, que pode ser a resposta do RAG, um pedido de mais informações, ou a confirmação de abertura de chamado.\n",
        "if resposta_final.get(\"citacoes\"): ...: Verifica se há citações no estado final (resposta_final). Se houver (resposta_final.get(\"citacoes\") retornar uma lista não vazia), ele imprime as citações.\n",
        "print(\"CITAÇÕES:\"): Imprime o cabeçalho para a seção de citações.\n",
        "for citacao in resposta_final.get(\"citacoes\"): ...: Itera sobre a lista de citações.\n",
        "print(f\" - Documento: {citacao['documento']}, Página: {citacao['pagina']}\"): Imprime o nome do documento e o número da página da citação.\n",
        "print(f\" Trecho: {citacao['trecho']}\"): Imprime o trecho relevante do documento citado.\n",
        "print(\"------------------------------------\"): Imprime uma linha separadora para distinguir os resultados de cada pergunta de teste.\n",
        "Em resumo, este código pega uma lista de perguntas, executa cada uma delas através do grafo do Langgraph, e depois imprime um resumo detalhado do resultado, incluindo a decisão de triagem, a ação final, a resposta gerada e, se aplicável, as citações relevantes dos documentos usados pelo RAG. Isso permite que você veja como o seu agente se comporta com diferentes tipos de entradas.\n",
        "\n"
      ],
      "metadata": {
        "id": "zf6xII0aPl7M"
      }
    }
  ]
}